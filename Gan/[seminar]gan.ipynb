{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2178,
     "status": "ok",
     "timestamp": 1576358120102,
     "user": {
      "displayName": "Кирилл Игоревич Голубев",
      "photoUrl": "",
      "userId": "04104925801798209408"
     },
     "user_tz": -180
    },
    "id": "SXn1O59KHSjS",
    "outputId": "0eeb963a-2c35-4285-a1dc-69538f8dbfa5"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named &#39;keras&#39;",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m&lt;ipython-input-1-d8665893ce5d&gt;\u001b[0m in \u001b[0;36m&lt;module&gt;\u001b[1;34m\u001b[0m\n\u001b[1;32m----&gt; 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZeroPadding2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvanced_activations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolutional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUpSampling2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named &#39;keras&#39;"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qW1bz9alzJTd"
   },
   "source": [
    "для развлечений:\n",
    "[dreamer.py](https://github.com/samim23/DeepDreamAnim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bAurW_43ka0n"
   },
   "source": [
    "Реализация GAN'а ниже взята [отсюда](https://github.com/eriklindernoren/Keras-GAN)<br />\n",
    "Ее вывод будет сохраняться в папку images. <br \\> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ccCld37tHfvs"
   },
   "outputs": [],
   "source": [
    "!mkdir images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vpd2LRTeHSjt"
   },
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminatorч\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                # Plot the progress\n",
    "                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 885
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1495,
     "status": "ok",
     "timestamp": 1576358175759,
     "user": {
      "displayName": "Кирилл Игоревич Голубев",
      "photoUrl": "",
      "userId": "04104925801798209408"
     },
     "user_tz": -180
    },
    "id": "SEr7U1LcHSj3",
    "outputId": "e6bf0cfb-3837-4a27-87f9-4ba7bbbb6ef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan = GAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 333,
     "status": "ok",
     "timestamp": 1545486087158,
     "user": {
      "displayName": "Кирилл Игоревич Голубев",
      "photoUrl": "",
      "userId": "04104925801798209408"
     },
     "user_tz": -180
    },
    "id": "QdgeeExSHSkG",
    "outputId": "bea88866-20b2-4973-dbdb-b339a6b99a7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.017927, acc.: 16.02%] [G loss: 0.631142]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 [D loss: 0.033250, acc.: 100.00%] [G loss: 3.502398]\n",
      "100 [D loss: 0.231046, acc.: 91.02%] [G loss: 2.921677]\n",
      "150 [D loss: 0.374722, acc.: 81.64%] [G loss: 2.054330]\n",
      "200 [D loss: 0.364900, acc.: 77.34%] [G loss: 3.028027]\n",
      "250 [D loss: 0.640153, acc.: 67.58%] [G loss: 1.787613]\n",
      "300 [D loss: 0.691566, acc.: 43.75%] [G loss: 0.845001]\n",
      "350 [D loss: 0.689415, acc.: 46.88%] [G loss: 0.683935]\n",
      "400 [D loss: 0.666428, acc.: 48.05%] [G loss: 0.720757]\n",
      "450 [D loss: 0.630760, acc.: 59.77%] [G loss: 0.783331]\n",
      "500 [D loss: 0.620490, acc.: 69.53%] [G loss: 0.790576]\n",
      "550 [D loss: 0.630585, acc.: 66.41%] [G loss: 0.784604]\n",
      "600 [D loss: 0.642089, acc.: 65.23%] [G loss: 0.789346]\n",
      "650 [D loss: 0.636837, acc.: 61.33%] [G loss: 0.854472]\n",
      "700 [D loss: 0.643464, acc.: 63.67%] [G loss: 0.865808]\n",
      "750 [D loss: 0.585008, acc.: 75.78%] [G loss: 0.891413]\n",
      "800 [D loss: 0.615263, acc.: 66.80%] [G loss: 0.871182]\n",
      "850 [D loss: 0.577003, acc.: 78.12%] [G loss: 0.903152]\n",
      "900 [D loss: 0.604362, acc.: 68.36%] [G loss: 0.941538]\n",
      "950 [D loss: 0.610266, acc.: 61.72%] [G loss: 0.901455]\n",
      "1000 [D loss: 0.615684, acc.: 66.80%] [G loss: 0.975157]\n",
      "1050 [D loss: 0.615532, acc.: 58.98%] [G loss: 1.004092]\n",
      "1100 [D loss: 0.619684, acc.: 70.70%] [G loss: 0.964012]\n",
      "1150 [D loss: 0.548597, acc.: 78.12%] [G loss: 0.980145]\n",
      "1200 [D loss: 0.587190, acc.: 71.88%] [G loss: 0.945925]\n",
      "1250 [D loss: 0.603152, acc.: 69.92%] [G loss: 0.959239]\n",
      "1300 [D loss: 0.605301, acc.: 65.23%] [G loss: 0.980249]\n",
      "1350 [D loss: 0.567841, acc.: 77.34%] [G loss: 1.015492]\n",
      "1400 [D loss: 0.588414, acc.: 71.88%] [G loss: 0.959346]\n",
      "1450 [D loss: 0.596749, acc.: 69.53%] [G loss: 0.920100]\n",
      "1500 [D loss: 0.559474, acc.: 77.73%] [G loss: 0.935760]\n",
      "1550 [D loss: 0.595825, acc.: 70.31%] [G loss: 0.942748]\n",
      "1600 [D loss: 0.608812, acc.: 68.75%] [G loss: 0.918284]\n",
      "1650 [D loss: 0.630369, acc.: 66.02%] [G loss: 0.896243]\n",
      "1700 [D loss: 0.610705, acc.: 69.14%] [G loss: 0.932568]\n",
      "1750 [D loss: 0.566191, acc.: 76.17%] [G loss: 0.988697]\n",
      "1800 [D loss: 0.569078, acc.: 71.09%] [G loss: 0.980477]\n",
      "1850 [D loss: 0.602290, acc.: 69.92%] [G loss: 0.985280]\n",
      "1900 [D loss: 0.592022, acc.: 71.48%] [G loss: 0.947828]\n",
      "1950 [D loss: 0.619511, acc.: 66.41%] [G loss: 0.941516]\n",
      "2000 [D loss: 0.641425, acc.: 64.06%] [G loss: 0.901600]\n",
      "2050 [D loss: 0.585740, acc.: 73.05%] [G loss: 0.930027]\n",
      "2100 [D loss: 0.638115, acc.: 64.84%] [G loss: 0.919905]\n",
      "2150 [D loss: 0.610772, acc.: 65.23%] [G loss: 0.857917]\n",
      "2200 [D loss: 0.572762, acc.: 77.34%] [G loss: 0.955525]\n",
      "2250 [D loss: 0.624085, acc.: 63.67%] [G loss: 0.918094]\n",
      "2300 [D loss: 0.590995, acc.: 72.66%] [G loss: 0.882920]\n",
      "2350 [D loss: 0.591994, acc.: 71.09%] [G loss: 0.871602]\n",
      "2400 [D loss: 0.603577, acc.: 70.31%] [G loss: 0.905873]\n",
      "2450 [D loss: 0.591589, acc.: 72.27%] [G loss: 0.873235]\n",
      "2500 [D loss: 0.600936, acc.: 68.36%] [G loss: 0.879352]\n",
      "2550 [D loss: 0.611045, acc.: 64.06%] [G loss: 0.902252]\n",
      "2600 [D loss: 0.625525, acc.: 65.23%] [G loss: 0.887127]\n",
      "2650 [D loss: 0.595763, acc.: 73.05%] [G loss: 0.838990]\n",
      "2700 [D loss: 0.587222, acc.: 72.66%] [G loss: 0.910945]\n",
      "2750 [D loss: 0.585990, acc.: 75.00%] [G loss: 0.907534]\n",
      "2800 [D loss: 0.625511, acc.: 68.36%] [G loss: 0.895388]\n",
      "2850 [D loss: 0.596291, acc.: 72.27%] [G loss: 0.926863]\n",
      "2900 [D loss: 0.591533, acc.: 71.48%] [G loss: 0.902349]\n",
      "2950 [D loss: 0.614766, acc.: 67.19%] [G loss: 0.860618]\n"
     ]
    }
   ],
   "source": [
    "gan.train(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PvG1bhHLHSkR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[seminar]gan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}